[package]
name = "llama-cpp"
authors = ["Rakuto Furutani"]
description = "Rust bindings for Llama.cpp"
license = "MIT"
version = "0.1.0"
edition = "2021"

[features]
default = ["huggingface", "stream"]
stream = ["dep:async-stream", "dep:futures-util"]
huggingface = ["dep:hf-hub"]

[[example]]
name = "simple_chat"
required-features = ["huggingface", "stream"]

[[example]]
name = "chat_lora"

[[example]]
name = "server"

[dependencies]
miette = { version = "7.2.0", features = ["fancy"] }
tokio = { version = "1", features = ["default"] }
async-stream = { version = "0.3.6", optional = true }
futures-util = { version = "0.3", optional = true }
lazy_static = "1.5.0"
log = "0.4.22"
hf-hub = { version = "0.3.2", features = ["tokio"], optional = true }
thiserror = "2.0"

[dev-dependencies]
clap = { version = "4.5", features = ["derive"] }
rustyline = "14.0.0"
axum = "0.7.7"
axum-valid = "0.21"
tower-http = { version = "0.6", features = ["cors"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tracing = "0.1"
tracing-subscriber = "0.3"

[build-dependencies]
cmake = "0.1"
bindgen = "0.70"
miette = { version = "7.2.0", features = ["fancy"] }
